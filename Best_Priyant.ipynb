{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMp1V/Pb/fHgv4/NfiOEtUy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pbhacks/AiGen/blob/main/Best_Priyant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VrUvJBh-Xfm9",
        "outputId": "b9a1e5c0-48c8-491d-8114-28cccda06da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.7)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.14.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Loading data...\n",
            "Preprocessing data...\n",
            "Running feature selection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 04:46:08,859] A new study created in memory with name: no-name-2f078db5-ca01-44ca-9338-d115bdcbb285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected 37 features out of 71\n",
            "Optimizing LightGBM model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-02-25 04:46:14,393] Trial 0 finished with value: 3.468801393229427 and parameters: {'num_leaves': 145, 'max_depth': 6, 'learning_rate': 0.011479376336363895, 'n_estimators': 687, 'subsample': 0.6120777962188625, 'colsample_bytree': 0.6519284241138991, 'reg_lambda': 4.412414704580997, 'reg_alpha': 0.6740782634109747}. Best is trial 0 with value: 3.468801393229427.\n",
            "[I 2025-02-25 04:46:15,764] Trial 1 finished with value: 5.28440067380212 and parameters: {'num_leaves': 66, 'max_depth': 8, 'learning_rate': 0.025714404267908407, 'n_estimators': 101, 'subsample': 0.660157237637398, 'colsample_bytree': 0.7156375745166882, 'reg_lambda': 0.1255045058976736, 'reg_alpha': 1.947606679789437}. Best is trial 0 with value: 3.468801393229427.\n",
            "[I 2025-02-25 04:46:19,261] Trial 2 finished with value: 3.817155339465001 and parameters: {'num_leaves': 123, 'max_depth': 7, 'learning_rate': 0.034124862190485744, 'n_estimators': 849, 'subsample': 0.6182021443858142, 'colsample_bytree': 0.627760387136652, 'reg_lambda': 2.184651322003162, 'reg_alpha': 1.0086683611874288}. Best is trial 0 with value: 3.468801393229427.\n",
            "[I 2025-02-25 04:46:19,658] Trial 3 finished with value: 2.336191157638208 and parameters: {'num_leaves': 91, 'max_depth': 3, 'learning_rate': 0.07255572138573113, 'n_estimators': 329, 'subsample': 0.7209294246375855, 'colsample_bytree': 0.9813313724209274, 'reg_lambda': 3.311208917905888, 'reg_alpha': 0.13612481590450956}. Best is trial 3 with value: 2.336191157638208.\n",
            "[I 2025-02-25 04:46:23,508] Trial 4 finished with value: 3.2800700383450927 and parameters: {'num_leaves': 136, 'max_depth': 11, 'learning_rate': 0.020033187148988236, 'n_estimators': 262, 'subsample': 0.9091690034352959, 'colsample_bytree': 0.758857742433974, 'reg_lambda': 0.3607283148476798, 'reg_alpha': 2.5796585837202306}. Best is trial 3 with value: 2.336191157638208.\n",
            "[I 2025-02-25 04:46:26,184] Trial 5 finished with value: 2.656738482137726 and parameters: {'num_leaves': 68, 'max_depth': 10, 'learning_rate': 0.03204391694866996, 'n_estimators': 428, 'subsample': 0.8701559292669186, 'colsample_bytree': 0.8708796689702158, 'reg_lambda': 0.5466865102516456, 'reg_alpha': 2.6206879009547532}. Best is trial 3 with value: 2.336191157638208.\n",
            "[I 2025-02-25 04:46:26,983] Trial 6 finished with value: 2.4938805143230565 and parameters: {'num_leaves': 69, 'max_depth': 7, 'learning_rate': 0.07997413031721778, 'n_estimators': 257, 'subsample': 0.8788492617771715, 'colsample_bytree': 0.8684658347567262, 'reg_lambda': 7.775905452607, 'reg_alpha': 0.2130749890096201}. Best is trial 3 with value: 2.336191157638208.\n",
            "[I 2025-02-25 04:46:28,158] Trial 7 finished with value: 3.0102199801575025 and parameters: {'num_leaves': 120, 'max_depth': 4, 'learning_rate': 0.04045098508040077, 'n_estimators': 667, 'subsample': 0.9579939690571921, 'colsample_bytree': 0.7233688444499271, 'reg_lambda': 6.5448572627621955, 'reg_alpha': 0.15939927925439987}. Best is trial 3 with value: 2.336191157638208.\n",
            "[I 2025-02-25 04:46:31,348] Trial 8 finished with value: 2.386003834762887 and parameters: {'num_leaves': 29, 'max_depth': 8, 'learning_rate': 0.023145042149672057, 'n_estimators': 855, 'subsample': 0.9032619846786976, 'colsample_bytree': 0.9235022920137174, 'reg_lambda': 0.7651858704759512, 'reg_alpha': 8.667882055360339}. Best is trial 3 with value: 2.336191157638208.\n",
            "[I 2025-02-25 04:46:32,518] Trial 9 finished with value: 2.630026262275202 and parameters: {'num_leaves': 132, 'max_depth': 11, 'learning_rate': 0.09623688181938599, 'n_estimators': 395, 'subsample': 0.9556947940124388, 'colsample_bytree': 0.8127420610487879, 'reg_lambda': 0.11077963608914002, 'reg_alpha': 0.35307175212751807}. Best is trial 3 with value: 2.336191157638208.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== LightGBM Hyperparameter Tuning Results ==========\n",
            "Best Parameters: {'num_leaves': 91, 'max_depth': 3, 'learning_rate': 0.07255572138573113, 'n_estimators': 329, 'subsample': 0.7209294246375855, 'colsample_bytree': 0.9813313724209274, 'reg_lambda': 3.311208917905888, 'reg_alpha': 0.13612481590450956}\n",
            "Best Trial MAE: 2.3362\n",
            "Training ensemble model...\n",
            "Training LightGBM model...\n",
            "Training XGBoost model...\n",
            "Training CatBoost model...\n",
            "Training Random Forest model...\n",
            "Training ensemble model...\n",
            "\n",
            "========== Model Evaluation ==========\n",
            "LightGBM MAE: 2.1204\n",
            "XGBoost MAE: 1.9470\n",
            "CatBoost MAE: 2.2915\n",
            "Random Forest MAE: 1.2039\n",
            "Ensemble MAE: 1.6335\n",
            "\n",
            "Best Model: Random Forest with MAE: 1.2039\n",
            "\n",
            "Generating final predictions...\n",
            "\n",
            "✅ Submission file saved as 'submission.csv'\n",
            "✅ Model achieved validation MAE of 1.2039\n",
            "✅ Model performance metrics saved to 'model_performance.txt'\n",
            "⏱️ Total execution time: 1 minutes and 3.57 seconds\n",
            "\n",
            "Top 10 Feature Importance:\n",
            "Heart_Risk_Index_target_mean: 0.9842\n",
            "Iron_Concentration_target_mean: 0.0061\n",
            "Energy_Content_times_Cholesterol_Level: 0.0010\n",
            "Energy_Content: 0.0009\n",
            "VitA_Percentage_target_mean: 0.0006\n",
            "Energy_Content_log: 0.0006\n",
            "Protein_Quantity: 0.0005\n",
            "Bone_Health_Calcium_target_mean: 0.0005\n",
            "Stimulant_Caffeine: 0.0004\n",
            "Saturated_Lipids_times_Fibre_Amount: 0.0003\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_93da84e7-7ff2-4ce8-9958-45dcd6176b9e\", \"submission.csv\", 23293)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_041bb4cf-ec3f-4b82-b395-04e5d0df69f2\", \"model_performance.txt\", 467)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Please note that kindly give it sometime after running the code for trials to complete and the submission file with parameters will auto download\n",
        "# Scroll at bottom most of this page to check status of code compilation\n",
        "# Install required packages\n",
        "!pip install pandas numpy scikit-learn lightgbm xgboost catboost optuna joblib\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "import catboost as cb\n",
        "from sklearn.ensemble import VotingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import optuna\n",
        "import warnings\n",
        "from joblib import Parallel, delayed\n",
        "import time\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Set output paths\n",
        "OUTPUT_DIR = '/content'\n",
        "SUBMISSION_PATH = os.path.join(OUTPUT_DIR, 'submission.csv')\n",
        "PERFORMANCE_PATH = os.path.join(OUTPUT_DIR, 'model_performance.txt')\n",
        "\n",
        "# Track execution time\n",
        "start_time = time.time()\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Loading data...\")\n",
        "# Load data\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Fix column names (strip spaces)\n",
        "train_df.columns = train_df.columns.str.strip()\n",
        "test_df.columns = test_df.columns.str.strip()\n",
        "\n",
        "# Ensure correct target column name\n",
        "target_col = \"Fat_Content\"\n",
        "if target_col not in train_df.columns:\n",
        "    raise KeyError(f\"Column '{target_col}' not found in training dataset. Check column names.\")\n",
        "\n",
        "# Save test IDs for submission\n",
        "test_ids = test_df[\"Id\"]\n",
        "\n",
        "print(\"Preprocessing data...\")\n",
        "# Feature Engineering - Optimized to create fewer but more meaningful features\n",
        "def create_features(df, is_train=True):\n",
        "    df_new = df.copy()\n",
        "\n",
        "    # Create numerical feature interactions - only the most important ones\n",
        "    numeric_cols = df_new.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    if 'Id' in numeric_cols:\n",
        "        numeric_cols.remove('Id')\n",
        "\n",
        "    # For training data, remove target from numeric cols for feature engineering\n",
        "    if is_train and target_col in numeric_cols:\n",
        "        numeric_cols.remove(target_col)\n",
        "\n",
        "    # Reduced set of transformations - focus on the most useful ones\n",
        "    for col in numeric_cols:\n",
        "        df_new[f'{col}_log'] = np.log1p(df_new[col].clip(lower=0))\n",
        "\n",
        "    # Create fewer interactions - only between pairs likely to be informative\n",
        "    # Limit to top 5 numeric columns if there are many\n",
        "    top_numeric = numeric_cols[:min(5, len(numeric_cols))]\n",
        "    for i in range(len(top_numeric)):\n",
        "        for j in range(i+1, len(top_numeric)):\n",
        "            col1, col2 = top_numeric[i], top_numeric[j]\n",
        "            df_new[f'{col1}_times_{col2}'] = df_new[col1] * df_new[col2]\n",
        "            df_new[f'{col1}_div_{col2}'] = df_new[col1] / (df_new[col2] + 1e-5)\n",
        "\n",
        "    return df_new\n",
        "\n",
        "# Create new features\n",
        "train_df = create_features(train_df, is_train=True)\n",
        "test_df = create_features(test_df, is_train=False)\n",
        "\n",
        "# Handle missing values in training and test data with more efficient methods\n",
        "for col in train_df.columns:\n",
        "    if col != target_col and col != 'Id':\n",
        "        if train_df[col].dtype == \"object\":\n",
        "            most_frequent = train_df[col].mode()[0]\n",
        "            train_df[col] = train_df[col].fillna(most_frequent)\n",
        "        else:\n",
        "            train_df[col] = train_df[col].fillna(train_df[col].median())\n",
        "\n",
        "for col in test_df.columns:\n",
        "    if col != 'Id':\n",
        "        if test_df[col].dtype == \"object\":\n",
        "            most_frequent = test_df[col].mode()[0] if not test_df[col].mode().empty else \"Unknown\"\n",
        "            test_df[col] = test_df[col].fillna(most_frequent)\n",
        "        else:\n",
        "            test_df[col] = test_df[col].fillna(test_df[col].median())\n",
        "\n",
        "# Advanced categorical encoding - focus on the most effective techniques\n",
        "categorical_cols = [col for col in train_df.select_dtypes(include=['object']).columns\n",
        "                    if col != target_col and col in test_df.columns]\n",
        "\n",
        "# Target encoding for categorical features\n",
        "for col in categorical_cols:\n",
        "    # Calculate target mean for each category\n",
        "    target_means = train_df.groupby(col)[target_col].mean()\n",
        "\n",
        "    # Map these means to both train and test data\n",
        "    train_df[f'{col}_target_mean'] = train_df[col].map(target_means)\n",
        "    test_df[f'{col}_target_mean'] = test_df[col].map(target_means)\n",
        "\n",
        "    # Fill missing values in the new column\n",
        "    train_df[f'{col}_target_mean'] = train_df[f'{col}_target_mean'].fillna(train_df[target_col].mean())\n",
        "    test_df[f'{col}_target_mean'] = test_df[f'{col}_target_mean'].fillna(train_df[target_col].mean())\n",
        "\n",
        "# Label encoding for categorical features\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    train_df[col] = le.fit_transform(train_df[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "    if col in test_df.columns:\n",
        "        # Handle unseen categories in test set\n",
        "        test_categories = set(test_df[col].astype(str).unique())\n",
        "        train_categories = set(le.classes_)\n",
        "        new_categories = test_categories - train_categories\n",
        "\n",
        "        if new_categories:\n",
        "            test_df[col] = test_df[col].apply(lambda x: str(x) if str(x) in train_categories else 'Unknown')\n",
        "            # Re-fit with the new 'Unknown' category if needed\n",
        "            if 'Unknown' not in train_categories:\n",
        "                train_df.loc[len(train_df)] = train_df.iloc[0].copy()\n",
        "                train_df.loc[len(train_df)-1, col] = 'Unknown'\n",
        "                le = LabelEncoder()\n",
        "                train_df[col] = le.fit_transform(train_df[col].astype(str))\n",
        "                label_encoders[col] = le\n",
        "\n",
        "        test_df[col] = test_df[col].astype(str).map(\n",
        "            lambda x: le.transform([x])[0] if x in le.classes_ else le.transform(['Unknown'])[0]\n",
        "        )\n",
        "\n",
        "# Ensure both datasets have the same columns (except target and Id)\n",
        "train_features = [col for col in train_df.columns if col != target_col and col != 'Id']\n",
        "test_features = [col for col in test_df.columns if col != 'Id']\n",
        "\n",
        "# Find columns in train but not in test\n",
        "for col in train_features:\n",
        "    if col not in test_features:\n",
        "        test_df[col] = 0  # Add missing columns to test\n",
        "\n",
        "# Find columns in test but not in train\n",
        "X_cols = [col for col in train_df.columns if col != target_col and col != 'Id']\n",
        "for col in test_features:\n",
        "    if col not in X_cols and col != 'Id':\n",
        "        train_df[col] = 0  # Add missing columns to train\n",
        "\n",
        "# Scale numeric features\n",
        "numeric_cols = [col for col in train_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "                if col != target_col and col != 'Id']\n",
        "common_numeric_cols = [col for col in numeric_cols if col in test_df.columns]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_df[common_numeric_cols] = scaler.fit_transform(train_df[common_numeric_cols])\n",
        "test_df[common_numeric_cols] = scaler.transform(test_df[common_numeric_cols])\n",
        "\n",
        "# Split data\n",
        "X = train_df.drop(columns=[target_col, 'Id'] if 'Id' in train_df.columns else [target_col])\n",
        "y = train_df[target_col]\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Ensure X_train and test_df have the same columns for prediction\n",
        "common_columns = [col for col in X_train.columns if col in test_df.columns]\n",
        "test_df_features = test_df[common_columns]\n",
        "\n",
        "print(\"Running feature selection...\")\n",
        "# Feature selection using LightGBM importance - with faster params\n",
        "initial_model = lgb.LGBMRegressor(random_state=42, n_estimators=100, verbose=-1)\n",
        "initial_model.fit(X_train, y_train)\n",
        "feature_selector = SelectFromModel(initial_model, threshold='median')\n",
        "feature_selector.fit(X_train, y_train)\n",
        "X_train_selected = feature_selector.transform(X_train)\n",
        "X_valid_selected = feature_selector.transform(X_valid)\n",
        "test_df_selected = feature_selector.transform(test_df_features)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features_mask = feature_selector.get_support()\n",
        "selected_features = [feature for feature, selected in zip(X_train.columns, selected_features_mask) if selected]\n",
        "print(f\"Selected {len(selected_features)} features out of {len(X_train.columns)}\")\n",
        "\n",
        "print(\"Optimizing LightGBM model...\")\n",
        "# Use faster objective function with fewer folds and early stopping\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'objective': 'regression',\n",
        "        'metric': 'mae',\n",
        "        'verbosity': -1,\n",
        "        'boosting_type': 'gbdt',\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-1, 10.0, log=True),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-1, 10.0, log=True),\n",
        "        'random_state': 42\n",
        "    }\n",
        "\n",
        "    # Use 3-fold CV instead of 5-fold for faster execution\n",
        "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    cv_scores = []\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_train_selected):\n",
        "        X_fold_train, X_fold_val = X_train_selected[train_idx], X_train_selected[val_idx]\n",
        "        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model = lgb.LGBMRegressor(**params)\n",
        "        model.fit(\n",
        "            X_fold_train, y_fold_train,\n",
        "            eval_set=[(X_fold_val, y_fold_val)],\n",
        "            eval_metric='mae',\n",
        "            callbacks=[lgb.early_stopping(20, verbose=False)]  # More aggressive early stopping\n",
        "        )\n",
        "\n",
        "        preds = model.predict(X_fold_val)\n",
        "        mae = mean_absolute_error(y_fold_val, preds)\n",
        "        cv_scores.append(mae)\n",
        "\n",
        "    return np.mean(cv_scores)\n",
        "\n",
        "# Run Optuna tuning with fewer trials\n",
        "study = optuna.create_study(direction=\"minimize\")\n",
        "study.optimize(objective, n_trials=10)  # Further reduced for Colab\n",
        "\n",
        "# Get best parameters\n",
        "best_params = study.best_params\n",
        "print(\"\\n========== LightGBM Hyperparameter Tuning Results ==========\")\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Best Trial MAE: {study.best_trial.value:.4f}\")\n",
        "\n",
        "print(\"Training ensemble model...\")\n",
        "# Train models in parallel using joblib\n",
        "def train_model(model_type, params, X_train, y_train):\n",
        "    if model_type == 'lgb':\n",
        "        model = lgb.LGBMRegressor(**params, random_state=42)\n",
        "    elif model_type == 'xgb':\n",
        "        model = xgb.XGBRegressor(**params, random_state=42)\n",
        "    elif model_type == 'cb':\n",
        "        model = cb.CatBoostRegressor(**params, random_seed=42, verbose=0)\n",
        "    elif model_type == 'rf':\n",
        "        model = RandomForestRegressor(**params, random_state=42)\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        "\n",
        "# 1. LightGBM parameters\n",
        "lgb_params = best_params\n",
        "\n",
        "# 2. XGBoost parameters\n",
        "xgb_params = {\n",
        "    'n_estimators': min(best_params['n_estimators'], 500),  # Cap at 500 for speed\n",
        "    'max_depth': best_params['max_depth'],\n",
        "    'learning_rate': best_params['learning_rate'],\n",
        "    'subsample': best_params['subsample'],\n",
        "    'colsample_bytree': best_params['colsample_bytree'],\n",
        "    'reg_lambda': best_params['reg_lambda'],\n",
        "    'reg_alpha': best_params['reg_alpha']\n",
        "}\n",
        "\n",
        "# 3. CatBoost parameters\n",
        "cb_params = {\n",
        "    'iterations': min(best_params['n_estimators'], 300),  # Cap at 300 for Colab\n",
        "    'depth': best_params['max_depth'],\n",
        "    'learning_rate': best_params['learning_rate']\n",
        "}\n",
        "\n",
        "# 4. Random Forest parameters\n",
        "rf_params = {\n",
        "    'n_estimators': 80,  # Further reduced for Colab\n",
        "    'max_depth': 10,\n",
        "    'min_samples_split': 5,\n",
        "    'min_samples_leaf': 4\n",
        "}\n",
        "\n",
        "# Train models one by one (more reliable in Colab environment)\n",
        "print(\"Training LightGBM model...\")\n",
        "lgb_model = train_model('lgb', lgb_params, X_train_selected, y_train)\n",
        "\n",
        "print(\"Training XGBoost model...\")\n",
        "xgb_model = train_model('xgb', xgb_params, X_train_selected, y_train)\n",
        "\n",
        "print(\"Training CatBoost model...\")\n",
        "cb_model = train_model('cb', cb_params, X_train_selected, y_train)\n",
        "\n",
        "print(\"Training Random Forest model...\")\n",
        "rf_model = train_model('rf', rf_params, X_train_selected, y_train)\n",
        "\n",
        "# Create a voting ensemble\n",
        "ensemble = VotingRegressor([\n",
        "    ('lgb', lgb_model),\n",
        "    ('xgb', xgb_model),\n",
        "    ('cb', cb_model),\n",
        "    ('rf', rf_model)\n",
        "])\n",
        "print(\"Training ensemble model...\")\n",
        "ensemble.fit(X_train_selected, y_train)\n",
        "\n",
        "# Evaluate models\n",
        "print(\"\\n========== Model Evaluation ==========\")\n",
        "lgb_preds = lgb_model.predict(X_valid_selected)\n",
        "xgb_preds = xgb_model.predict(X_valid_selected)\n",
        "cb_preds = cb_model.predict(X_valid_selected)\n",
        "rf_preds = rf_model.predict(X_valid_selected)\n",
        "ensemble_preds = ensemble.predict(X_valid_selected)\n",
        "\n",
        "# Calculate MAE for each model\n",
        "lgb_mae = mean_absolute_error(y_valid, lgb_preds)\n",
        "xgb_mae = mean_absolute_error(y_valid, xgb_preds)\n",
        "cb_mae = mean_absolute_error(y_valid, cb_preds)\n",
        "rf_mae = mean_absolute_error(y_valid, rf_preds)\n",
        "ensemble_mae = mean_absolute_error(y_valid, ensemble_preds)\n",
        "\n",
        "print(f\"LightGBM MAE: {lgb_mae:.4f}\")\n",
        "print(f\"XGBoost MAE: {xgb_mae:.4f}\")\n",
        "print(f\"CatBoost MAE: {cb_mae:.4f}\")\n",
        "print(f\"Random Forest MAE: {rf_mae:.4f}\")\n",
        "print(f\"Ensemble MAE: {ensemble_mae:.4f}\")\n",
        "\n",
        "# Choose the best model\n",
        "best_model_name = \"Ensemble\"\n",
        "best_model = ensemble\n",
        "best_mae = ensemble_mae\n",
        "\n",
        "if lgb_mae < best_mae:\n",
        "    best_model_name = \"LightGBM\"\n",
        "    best_model = lgb_model\n",
        "    best_mae = lgb_mae\n",
        "\n",
        "if xgb_mae < best_mae:\n",
        "    best_model_name = \"XGBoost\"\n",
        "    best_model = xgb_model\n",
        "    best_mae = xgb_mae\n",
        "\n",
        "if cb_mae < best_mae:\n",
        "    best_model_name = \"CatBoost\"\n",
        "    best_model = cb_model\n",
        "    best_mae = cb_mae\n",
        "\n",
        "if rf_mae < best_mae:\n",
        "    best_model_name = \"Random Forest\"\n",
        "    best_model = rf_model\n",
        "    best_mae = rf_mae\n",
        "\n",
        "print(f\"\\nBest Model: {best_model_name} with MAE: {best_mae:.4f}\")\n",
        "\n",
        "# Final prediction using the best model\n",
        "print(\"\\nGenerating final predictions...\")\n",
        "test_preds = best_model.predict(test_df_selected)\n",
        "\n",
        "# Create a weighted prediction by averaging with the original LightGBM model\n",
        "weighted_preds = 0.7 * test_preds + 0.3 * lgb_model.predict(test_df_selected)\n",
        "\n",
        "# Save submission file\n",
        "submission = pd.DataFrame({\"Id\": test_ids, \"Fat_Content\": weighted_preds})\n",
        "submission.to_csv(SUBMISSION_PATH, index=False)\n",
        "\n",
        "# Calculate execution time\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "minutes, seconds = divmod(execution_time, 60)\n",
        "\n",
        "# Save performance metrics\n",
        "with open(PERFORMANCE_PATH, \"w\") as f:\n",
        "    f.write(f\"Best Model: {best_model_name}\\n\")\n",
        "    f.write(f\"LightGBM Best Parameters: {best_params}\\n\")\n",
        "    f.write(f\"LightGBM MAE: {lgb_mae:.4f}\\n\")\n",
        "    f.write(f\"XGBoost MAE: {xgb_mae:.4f}\\n\")\n",
        "    f.write(f\"CatBoost MAE: {cb_mae:.4f}\\n\")\n",
        "    f.write(f\"Random Forest MAE: {rf_mae:.4f}\\n\")\n",
        "    f.write(f\"Ensemble MAE: {ensemble_mae:.4f}\\n\")\n",
        "    f.write(f\"Best Model MAE: {best_mae:.4f}\\n\")\n",
        "    f.write(f\"Total execution time: {int(minutes)} minutes and {seconds:.2f} seconds\\n\")\n",
        "\n",
        "print(\"\\n✅ Submission file saved as 'submission.csv'\")\n",
        "print(f\"✅ Model achieved validation MAE of {best_mae:.4f}\")\n",
        "print(f\"✅ Model performance metrics saved to 'model_performance.txt'\")\n",
        "print(f\"⏱️ Total execution time: {int(minutes)} minutes and {seconds:.2f} seconds\")\n",
        "\n",
        "# Print feature importance for the best model if it's a tree-based model\n",
        "if best_model_name in [\"LightGBM\", \"XGBoost\", \"CatBoost\", \"Random Forest\"]:\n",
        "    importance = None\n",
        "    if best_model_name == \"LightGBM\":\n",
        "        importance = best_model.feature_importances_\n",
        "    elif best_model_name == \"XGBoost\":\n",
        "        importance = best_model.feature_importances_\n",
        "    elif best_model_name == \"CatBoost\":\n",
        "        importance = best_model.feature_importances_\n",
        "    else:  # Random Forest\n",
        "        importance = best_model.feature_importances_\n",
        "\n",
        "    if importance is not None:\n",
        "        print(\"\\nTop 10 Feature Importance:\")\n",
        "        feature_importance = sorted(zip(selected_features, importance), key=lambda x: x[1], reverse=True)\n",
        "        for feature, importance in feature_importance[:10]:\n",
        "            print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "# Download the results\n",
        "files.download(SUBMISSION_PATH)\n",
        "files.download(PERFORMANCE_PATH)"
      ]
    }
  ]
}